{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/090_dlops_onnx/090_dlops_onnx.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLOps - ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado varios modelos, los hemos comparado y hemos decidido usar uno, tenemos que ponerlo en producción para que sea accesible. Para ello tenemos diferentes alternativas, en función de la aplicación (desde desplegar un modelo en dispositivos móviles o IoT hasta en servidores en la nube accesibles a través de una API). En esta serie de posts asumiermos que nuestro modelo será ejecutado en un servidor en la nube, lo cual es lo más común ya que de esta manera podemos controlar los recursos computacionales disponibles para su ejecución, monitorizarlo, desplegar nuevas versiones fácilmente, etc. De nuevo, para ello tenemos diferentes opciones. En nuestro caso, que hemos entrenado los modelos usando Pytorch y Pytorch Lightning, podríamos usar cualquier *framework* en Python que nos permita servir las predicciones a través de internet, como por ejemplo [Flask](https://flask.palletsprojects.com/en/2.0.x/) o [FastAPI](https://fastapi.tiangolo.com/). El principal problema de esta opción es que tendremos que cargar todas las librerías (y sus dependencias) en nuestra API, lo cual resultará en una carga muy pesada. Recientemente, Pytorch incluye una solución dedicada para este caso de uso, [Torchserve](https://pytorch.org/serve/) que si bien nos ofrece una solución optimizada para servir modelos en producción, está limitada al uso de modelos en Pytorch.\n",
    "\n",
    "Es en este punto en el que entra [ONNX](https://onnx.ai/), un estándar abierto para la representación de redes neuronales que permite la interoperabilidad entre librerías y ofrece una solución optimizada para servir modelos en producción (tanto en la nube como on dispositvos móviles). De esta manera podemos desacoplar el entrenamiento de los modelos de su puesta en producción, utilizando en cada caso las herramientas preferidas para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar un modelo a ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver como podemos exportar un modelo entrenado a ONNX. En primer lugar, cargaremos el *checkpoint* deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import *\n",
    "\n",
    "module = MNISTModule.load_from_checkpoint('checkpoints/006-val_loss=0.14715-epoch=7.ckpt')\n",
    "module.mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una buena práctica evaluar nuestro modelo antes y después de exportarlo para asegurarnos de que todo funciona correctamenete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949999988079071"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "dm = MNISTDataModule(**module.hparams['datamodule'])\n",
    "dm.setup()\n",
    "module.eval()\n",
    "with torch.no_grad():\n",
    "    preds, labels = torch.tensor([]), torch.tensor([])\n",
    "    for imgs, _labels in dm.val_dataloader():\n",
    "        outputs = module.predict(imgs) > 0.5\n",
    "        preds = torch.cat([preds, outputs.cpu().long()])\n",
    "        labels = torch.cat([labels, _labels])\n",
    "\n",
    "acc = (preds == labels).float().mean()\n",
    "acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning nos permite exportar un modelo a ONNX de manera muy sencilla con la siguiente línea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/miniconda3/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:688: UserWarning: ONNX export squeeze with negative axis -1 might cause the onnx model to be incorrect. Negative axis is not supported in ONNX. Axis is converted to 1 based on input shape at export time. Passing an tensor of different rank in execution will be incorrect.\n",
      "  warnings.warn(\"ONNX export squeeze with negative axis \" + str(squeeze_dim) +\n",
      "/home/juan/miniconda3/lib/python3.8/site-packages/torch/onnx/symbolic_opset9.py:714: UserWarning: This model contains a squeeze operation on dimension 1. If the model is intended to be used with dynamic input shapes, please use opset version 11 to export the model.\n",
      "  warnings.warn(\"This model contains a squeeze operation on dimension \" + str(squeeze_dim) + \". If the model is \" +\n"
     ]
    }
   ],
   "source": [
    "input_sample = torch.randn((1,28,28))\n",
    "module.to_onnx('models/006.onnx', input_sample, export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'onnxruntime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/juan/Desktop/dlops/04_onnx.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bpykon/home/juan/Desktop/dlops/04_onnx.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnxruntime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxruntime'"
     ]
    }
   ],
   "source": [
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74dbfc52f168b3071122cf9c0781887d6121c12f9c1b29bca56ce221bccb2a07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
